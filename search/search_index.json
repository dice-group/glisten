{"config":{"indexing":"full","lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"About Glisten is a benchmarking framework for linkage recommendation systems. What is a \"Linkage Recommendation System\"? A linkage recommendation system can analyze datasets and given another dataset, can tell you which of the analyzed datasets fits topically to the given dataset. It creates a score for each of the analyzed datasets which represents how well the datasets link to the given dataset. How does Glisten work? Glisten provides a core library, which can be executed as a CLI benchmark or can be used inside Hobbit. Glisten provides a source dataset and target datasets, which will be given to the recommendation system. The system should then respond with a ranking of the best fitting target dataset to the least fitting target datasets wrt linkage with the source dataset. Glisten then goes through the ranking, and adds one the recommendations after another to the source dataset. Each time a dataset was added, glisten uses a Scorer system (e.g. a fact checking system like Copaal) to create a score for the current dataset. The Idea is, that the facts will get better scores (and thus the scorer will produce a better overall score) when good fitting target datasets were added. Hence the earlier the good fitting datasets were added, the earlier the score will rise. To take this into account, Glisten uses a ROC curve and the corresponding AUC score as how good the recommendation system is. Each time the score gets better the ROC curve gets better, and hence the overall AUC score. More indepth Idea A Benchmark in Glisten is defined to check one or more source datasets against several target datasets. For each source dataset a recommendation system should get a score for each target dataset representing how well this dataset fits to the source. Hence we get a ranking of the target datasets from best fitting to least fitting. Glisten then uses Fact Checking to check if the ranking puts all the relevant datasets on top. It will generate some true and some wrong facts from the source dataset. Now using these facts the Fact Checking system will be used to check each fact against the source dataset. This represents the baseline. Now we add the highest ranked target dataset and check again. Ideally the fact checker could declare more true facts true and more wrong facts wrong. The fact checking score rises. Hence if the recommendation system rates the better fitting target datasets higher, the score the system will retrieve will be better than if the system ranks low fitting target datasets higher. A small Note! We don't directly add the target, but a precomputed linked dataset of the target with the source file (It contains the target, but also the links between the source and the target) What is Hobbit? Hobbit is a platform for holisitc benchmarking of Big Linked Data. It deploys a platform https://master.project-hobbit.eu that allows users to deploy and execute their benchmarks. The glisten benchmark is integrated into the platform and can be easily executed inside hobbit, without setting anything up. If you want to deploy your Recommendation System to be used in Hobbit, have a look at: How to add my System to Hobbit How to use the local benchmark execution? Sadly a prebuild release would be too big (about 500MB as it includes the Stanford NLP models) and thus cannot be downloaded directly, however you can build Glisten easily yourself. # Download Glisten git clone https://github.com/dice-group/Glisten cd Glisten # build glisten mvn clean package # Execute the Help option to see what you can do java -jar glisten-test-$RELEASE_VERSION.jar -h The help execution will print this screen. Usage: glisten-test [-hV] [--clean-up] [-c=<configFile>] [-F=<numberOfFalseStatements>] [-m=<maxRecommendations>] [--max-property-limit=<maxPropertyLimit>] [--min-prop-occ=<minPropOcc>] [-N=<benchmarkName>] [-o=<orderFile>] [-s=<scorerAlgorithm>] [-S=<seed>] [--sample-size=<sampleSize>] [-t=<threshold>] [-T=<numberOfTrueStatements>] <rdfEndpoint> Executes the glisten workflow without Hobbit and prints the ROC curve at the end. Mostly useful for debugging. Uses the load_triplestore.sh script file to upload datasets to the triplestore. <rdfEndpoint> the rdf endpoint to use -c, --config=<configFile> the config file for glisten. Default: data_config.yml --clean-up if set, will remove the testing directory which includes all downloaded and extracted datasets. -F, --no-of-false-stmts=<numberOfFalseStatements> the no. of false statements to generate. Default=5 -h, --help Show this help message and exit. -m, --max-recommendations=<maxRecommendations> the no. of max recommendations, 0 or lower means that all recommendations will be looked at. Default=10 --max-property-limit=<maxPropertyLimit> the maximum a property is allowed to be added for performance reasons. Default=30 --min-prop-occ=<minPropOcc> the minimum a property has to occur to be considered for the fact generation. Default=10 -N, --benchmark-name=<benchmarkName> The name of the benchmark to use. Name is specified inside the given configuration file. -o, --order-file=<orderFile> A file containing the order of the recommendations, if not set, will be random -s, --scorer=<scorerAlgorithm> The Scorer algorithm to use. Algorithms: [Copaal, SampleCopaal, Copaal_RootMeanSquare, SampleCopaal_RootMeanSquare, Copaal_AvgScore, SampleCopaal_AvgScore] -S, --seed=<seed> the seed to use for anything random we do. Default is random --sample-size=<sampleSize> the sample size to use if Scorer uses samples. Default=30 -t, --scorer-threshold=<threshold> the threshold to use inside the scorer. A true fact needs to be better than the threshold. Default=0.0 -T, --no-of-true-stmts=<numberOfTrueStatements> the no. of true statements to generate. Default=5 -V, --version Print version information and exit. Have a look at Configuration for further information. How do you calculate the Scorer scores? There are three different ways to calculate the score to use in the ROC cuve. All three ways are implement for both systems, Copaal as well as SampleCopaal For each fact copaal checks the veracity score. A score that basically means how \"true\" the fact seems. We generate some true and some false facts from the source model for this. Using ROC/AUC (default): We then sort the facts after their veracity scores and for each score we then check if the fact is a true fact or a false fact. If the fact is a true fact the ROC curve goes up, if it is a false fact the ROC curve goes right. Hence if an added target dataset provides some value for a true fact, the true fact should be seen as truer than before by Copaal. Thus it ideally goes up in the ranking, and the ROC curve is going up earlier, providing a better AUC score. Using Average Score (AvgScore) We simply take the average of the veracity scores for each. Hence if an added target dataset provides some value for a true fact, the veracity score gets better and the average score as well. Using Root Mean Square Error The same way the average score is calcualted except we calculate the Root Mean Square Error. Where is the code? The code is open source at https://github.com/dice-group/Glisten and you can code with us if you want to :) Where do I submit a bug or enhancement? Please use the Github Issue Tracker at https://github.com/dice-group/Glisten/issues","title":"About"},{"location":"#about","text":"Glisten is a benchmarking framework for linkage recommendation systems.","title":"About"},{"location":"#what-is-a-linkage-recommendation-system","text":"A linkage recommendation system can analyze datasets and given another dataset, can tell you which of the analyzed datasets fits topically to the given dataset. It creates a score for each of the analyzed datasets which represents how well the datasets link to the given dataset.","title":"What is a \"Linkage Recommendation System\"?"},{"location":"#how-does-glisten-work","text":"Glisten provides a core library, which can be executed as a CLI benchmark or can be used inside Hobbit. Glisten provides a source dataset and target datasets, which will be given to the recommendation system. The system should then respond with a ranking of the best fitting target dataset to the least fitting target datasets wrt linkage with the source dataset. Glisten then goes through the ranking, and adds one the recommendations after another to the source dataset. Each time a dataset was added, glisten uses a Scorer system (e.g. a fact checking system like Copaal) to create a score for the current dataset. The Idea is, that the facts will get better scores (and thus the scorer will produce a better overall score) when good fitting target datasets were added. Hence the earlier the good fitting datasets were added, the earlier the score will rise. To take this into account, Glisten uses a ROC curve and the corresponding AUC score as how good the recommendation system is. Each time the score gets better the ROC curve gets better, and hence the overall AUC score.","title":"How does Glisten work?"},{"location":"#more-indepth-idea","text":"A Benchmark in Glisten is defined to check one or more source datasets against several target datasets. For each source dataset a recommendation system should get a score for each target dataset representing how well this dataset fits to the source. Hence we get a ranking of the target datasets from best fitting to least fitting. Glisten then uses Fact Checking to check if the ranking puts all the relevant datasets on top. It will generate some true and some wrong facts from the source dataset. Now using these facts the Fact Checking system will be used to check each fact against the source dataset. This represents the baseline. Now we add the highest ranked target dataset and check again. Ideally the fact checker could declare more true facts true and more wrong facts wrong. The fact checking score rises. Hence if the recommendation system rates the better fitting target datasets higher, the score the system will retrieve will be better than if the system ranks low fitting target datasets higher. A small Note! We don't directly add the target, but a precomputed linked dataset of the target with the source file (It contains the target, but also the links between the source and the target)","title":"More indepth Idea"},{"location":"#what-is-hobbit","text":"Hobbit is a platform for holisitc benchmarking of Big Linked Data. It deploys a platform https://master.project-hobbit.eu that allows users to deploy and execute their benchmarks. The glisten benchmark is integrated into the platform and can be easily executed inside hobbit, without setting anything up. If you want to deploy your Recommendation System to be used in Hobbit, have a look at: How to add my System to Hobbit","title":"What is Hobbit?"},{"location":"#how-to-use-the-local-benchmark-execution","text":"Sadly a prebuild release would be too big (about 500MB as it includes the Stanford NLP models) and thus cannot be downloaded directly, however you can build Glisten easily yourself. # Download Glisten git clone https://github.com/dice-group/Glisten cd Glisten # build glisten mvn clean package # Execute the Help option to see what you can do java -jar glisten-test-$RELEASE_VERSION.jar -h The help execution will print this screen. Usage: glisten-test [-hV] [--clean-up] [-c=<configFile>] [-F=<numberOfFalseStatements>] [-m=<maxRecommendations>] [--max-property-limit=<maxPropertyLimit>] [--min-prop-occ=<minPropOcc>] [-N=<benchmarkName>] [-o=<orderFile>] [-s=<scorerAlgorithm>] [-S=<seed>] [--sample-size=<sampleSize>] [-t=<threshold>] [-T=<numberOfTrueStatements>] <rdfEndpoint> Executes the glisten workflow without Hobbit and prints the ROC curve at the end. Mostly useful for debugging. Uses the load_triplestore.sh script file to upload datasets to the triplestore. <rdfEndpoint> the rdf endpoint to use -c, --config=<configFile> the config file for glisten. Default: data_config.yml --clean-up if set, will remove the testing directory which includes all downloaded and extracted datasets. -F, --no-of-false-stmts=<numberOfFalseStatements> the no. of false statements to generate. Default=5 -h, --help Show this help message and exit. -m, --max-recommendations=<maxRecommendations> the no. of max recommendations, 0 or lower means that all recommendations will be looked at. Default=10 --max-property-limit=<maxPropertyLimit> the maximum a property is allowed to be added for performance reasons. Default=30 --min-prop-occ=<minPropOcc> the minimum a property has to occur to be considered for the fact generation. Default=10 -N, --benchmark-name=<benchmarkName> The name of the benchmark to use. Name is specified inside the given configuration file. -o, --order-file=<orderFile> A file containing the order of the recommendations, if not set, will be random -s, --scorer=<scorerAlgorithm> The Scorer algorithm to use. Algorithms: [Copaal, SampleCopaal, Copaal_RootMeanSquare, SampleCopaal_RootMeanSquare, Copaal_AvgScore, SampleCopaal_AvgScore] -S, --seed=<seed> the seed to use for anything random we do. Default is random --sample-size=<sampleSize> the sample size to use if Scorer uses samples. Default=30 -t, --scorer-threshold=<threshold> the threshold to use inside the scorer. A true fact needs to be better than the threshold. Default=0.0 -T, --no-of-true-stmts=<numberOfTrueStatements> the no. of true statements to generate. Default=5 -V, --version Print version information and exit. Have a look at Configuration for further information.","title":"How to use the local benchmark execution?"},{"location":"#how-do-you-calculate-the-scorer-scores","text":"There are three different ways to calculate the score to use in the ROC cuve. All three ways are implement for both systems, Copaal as well as SampleCopaal For each fact copaal checks the veracity score. A score that basically means how \"true\" the fact seems. We generate some true and some false facts from the source model for this.","title":"How do you calculate the Scorer scores?"},{"location":"#using-rocauc-default","text":"We then sort the facts after their veracity scores and for each score we then check if the fact is a true fact or a false fact. If the fact is a true fact the ROC curve goes up, if it is a false fact the ROC curve goes right. Hence if an added target dataset provides some value for a true fact, the true fact should be seen as truer than before by Copaal. Thus it ideally goes up in the ranking, and the ROC curve is going up earlier, providing a better AUC score.","title":"Using ROC/AUC (default):"},{"location":"#using-average-score-avgscore","text":"We simply take the average of the veracity scores for each. Hence if an added target dataset provides some value for a true fact, the veracity score gets better and the average score as well.","title":"Using Average Score (AvgScore)"},{"location":"#using-root-mean-square-error","text":"The same way the average score is calcualted except we calculate the Root Mean Square Error.","title":"Using Root Mean Square Error"},{"location":"#where-is-the-code","text":"The code is open source at https://github.com/dice-group/Glisten and you can code with us if you want to :)","title":"Where is the code?"},{"location":"#where-do-i-submit-a-bug-or-enhancement","text":"Please use the Github Issue Tracker at https://github.com/dice-group/Glisten/issues","title":"Where do I submit a bug or enhancement?"},{"location":"develop/core/","text":"This section explains how to change and extend the Core library. You can either add Glisten as library (you need to add glisten as a maven dependency in this case) or develop on the glisten code directly. Add Glisten as a Maven dependency If you just want to extend Glisten add the github repository to your pom.xml <repository> <id>glisten-github</id> <name>Glisten Dice Group repository</name> <url>https://maven.pkg.github.com/dice-group/Glisten</url> </repository> Now add the dependency <dependency> <groupId>org.dice_group</groupId> <artifactId>Glisten</artifactId> <version>$RELEASE_VERSION</version> </dependency> Add your own Scorer Create your Scorer Algorithm to use inside glisten. You need to Create your Scorer Algorithm Add the Algorithm to the ScorerFactory Create your Scorer Algorithm To create your Scorer simply extend the Scorer class located in org.dice_group.glisten.core.scorer and you only need to implement one function class MyScorer2000(namespaces: List<String>) : Scorer(namespaces){ /** * Gets the Scores for each Fact as a Pair whereas the returned Pairs consists of the original trueness Value as the first element * and the score as the second. * * @param endpoint the rdf endpoint to use * @param facts the facts to calculate the scores against, each pair consists of the statement and its trueness value * * @return a list of pairs containing the trueness value of the fact and the score */ override fun getScores(endpoint: String, facts: List<Pair<Statement, Double>>): MutableList<Pair<Double, Double>>{ //TODO } } As you can see you need to extend the getScores function, you get some facts and a SPARQL endpoint provided and you should create a list of Pairs representing the trueness value and your score. In other words if the fact at position 4 looks is Statement, 1.0 and your score for that Statement is f.e. 0.4 , then the list you're returing has to have the Pair 1.0, 0.4 at position 4 . That's it now let's add your Scorer algorithm to the ScorerFactory so we can use it by simply stating it in the Evaluation parameters. Add you Scorer to the ScorerFactory This step is pretty basic and you only need to name your algorithm like MyScorer2000 . Inside the ScorerFactory in org.dice_group.glisten.core.scorer.Scorer.kt add to the createOrDefault method your scorer name like fun createScorerOrDefault(scorerAlgorithm: String, namespaces: List<String>, seed: Long, sampleSize: Int) : Scorer { var scorer : Scorer = Copaal(namespaces) when(scorerAlgorithm.lowercase(Locale.getDefault())){ \"copaal\" -> scorer = Copaal(namespaces) \"myscorer2000\" -> scorer = MyScorer2000(namespaces) } return scorer } and add your Scorer name to the KDoc. Here you can see that you'll get some namespaces and a seed and a sampleSize if your algorithm needs that. Additionally you should add the name to the Test.kt file so Users can know that they can use your scorer. Do this by simply adding it to the Option description of the scorerAlgorithm parameter like \"The Scorer algorithm to use. Algorithms: [Copaal, SampleCopaal, MyScorer2000]\" Add your Scorer to Hobbit. If you want to use the Scorer inside Hobbit, you need to add the Scorer to the benchmark.ttl inside the Glisten Hobbit repository at TODO . Add the following to the benchmark.ttl TODO Add your own Fact Generation Base List Create a fact generation base list (or Statmenet Drawer) like the Allow list or Block list There are two steps involved. Create your Statement Drawer Add the Statment Drawer to the Configuration Create the Statment Drawer To create a Statement Drawer we simply need to extend the StmtDrawer class. Let's create our Drawer class MyDrawer2000(private val blockList: Collection<String>, private val seed: Long, override val model : Model, private val minPropOcc: Int, private val maxPropertyLimit: Int) : StmtDrawer(seed, model, minPropOcc, maxPropertyLimit) { //This is the heart of our Drawer override fun getStmts(): MutableList<Statement> { //create your statements here. and return them. } } Inside the getStmts() method we create and return the list of Statements to consider either for true or for false statement creation. If you want to completly rewrite how true and false statements are generated you need to change the TaskDrawer accordingly. Add your Drawer to the Configuration Now let's name our drawer MyDrawer2000 However we want to state our name inside the configuration just as we can state Allowlist or Blocklist as the type. To do this go to the Configurations file inside the package org.dice_group.glisten.core.config.Configuration . There us the Configuration class and the method createStmtDrawer inside this method you'll find a when statment. Add your Drawer as following private fun createStmtDrawer(type: String, list: Collection<String>, seed: Long, model: Model, minPropOcc: Int, maxPropertyLimit: Int ): StmtDrawer { //Add your new statement drawer type here to the `when` clause return when{ type.lowercase(Locale.getDefault()) == \"allowlist\" -> AllowListDrawer(list, seed, model, minPropOcc, maxPropertyLimit) type == \"MyDrawer2000\" -> MyDrawer2000(list, seed, model, minPropOcc, maxPropertyLimit) else -> BlockListDrawer(list, seed, model, minPropOcc, maxPropertyLimit) } } that is it. You can now use your Statement Drawer from the Configuration file.","title":"Core"},{"location":"develop/core/#add-glisten-as-a-maven-dependency","text":"If you just want to extend Glisten add the github repository to your pom.xml <repository> <id>glisten-github</id> <name>Glisten Dice Group repository</name> <url>https://maven.pkg.github.com/dice-group/Glisten</url> </repository> Now add the dependency <dependency> <groupId>org.dice_group</groupId> <artifactId>Glisten</artifactId> <version>$RELEASE_VERSION</version> </dependency>","title":"Add Glisten as a Maven dependency"},{"location":"develop/core/#add-your-own-scorer","text":"Create your Scorer Algorithm to use inside glisten. You need to Create your Scorer Algorithm Add the Algorithm to the ScorerFactory","title":"Add your own Scorer"},{"location":"develop/core/#create-your-scorer-algorithm","text":"To create your Scorer simply extend the Scorer class located in org.dice_group.glisten.core.scorer and you only need to implement one function class MyScorer2000(namespaces: List<String>) : Scorer(namespaces){ /** * Gets the Scores for each Fact as a Pair whereas the returned Pairs consists of the original trueness Value as the first element * and the score as the second. * * @param endpoint the rdf endpoint to use * @param facts the facts to calculate the scores against, each pair consists of the statement and its trueness value * * @return a list of pairs containing the trueness value of the fact and the score */ override fun getScores(endpoint: String, facts: List<Pair<Statement, Double>>): MutableList<Pair<Double, Double>>{ //TODO } } As you can see you need to extend the getScores function, you get some facts and a SPARQL endpoint provided and you should create a list of Pairs representing the trueness value and your score. In other words if the fact at position 4 looks is Statement, 1.0 and your score for that Statement is f.e. 0.4 , then the list you're returing has to have the Pair 1.0, 0.4 at position 4 . That's it now let's add your Scorer algorithm to the ScorerFactory so we can use it by simply stating it in the Evaluation parameters.","title":"Create your Scorer Algorithm"},{"location":"develop/core/#add-you-scorer-to-the-scorerfactory","text":"This step is pretty basic and you only need to name your algorithm like MyScorer2000 . Inside the ScorerFactory in org.dice_group.glisten.core.scorer.Scorer.kt add to the createOrDefault method your scorer name like fun createScorerOrDefault(scorerAlgorithm: String, namespaces: List<String>, seed: Long, sampleSize: Int) : Scorer { var scorer : Scorer = Copaal(namespaces) when(scorerAlgorithm.lowercase(Locale.getDefault())){ \"copaal\" -> scorer = Copaal(namespaces) \"myscorer2000\" -> scorer = MyScorer2000(namespaces) } return scorer } and add your Scorer name to the KDoc. Here you can see that you'll get some namespaces and a seed and a sampleSize if your algorithm needs that. Additionally you should add the name to the Test.kt file so Users can know that they can use your scorer. Do this by simply adding it to the Option description of the scorerAlgorithm parameter like \"The Scorer algorithm to use. Algorithms: [Copaal, SampleCopaal, MyScorer2000]\"","title":"Add you Scorer to the ScorerFactory"},{"location":"develop/core/#add-your-scorer-to-hobbit","text":"If you want to use the Scorer inside Hobbit, you need to add the Scorer to the benchmark.ttl inside the Glisten Hobbit repository at TODO . Add the following to the benchmark.ttl TODO","title":"Add your Scorer to Hobbit."},{"location":"develop/core/#add-your-own-fact-generation-base-list","text":"Create a fact generation base list (or Statmenet Drawer) like the Allow list or Block list There are two steps involved. Create your Statement Drawer Add the Statment Drawer to the Configuration","title":"Add your own Fact Generation Base List"},{"location":"develop/core/#create-the-statment-drawer","text":"To create a Statement Drawer we simply need to extend the StmtDrawer class. Let's create our Drawer class MyDrawer2000(private val blockList: Collection<String>, private val seed: Long, override val model : Model, private val minPropOcc: Int, private val maxPropertyLimit: Int) : StmtDrawer(seed, model, minPropOcc, maxPropertyLimit) { //This is the heart of our Drawer override fun getStmts(): MutableList<Statement> { //create your statements here. and return them. } } Inside the getStmts() method we create and return the list of Statements to consider either for true or for false statement creation. If you want to completly rewrite how true and false statements are generated you need to change the TaskDrawer accordingly.","title":"Create the Statment Drawer"},{"location":"develop/core/#add-your-drawer-to-the-configuration","text":"Now let's name our drawer MyDrawer2000 However we want to state our name inside the configuration just as we can state Allowlist or Blocklist as the type. To do this go to the Configurations file inside the package org.dice_group.glisten.core.config.Configuration . There us the Configuration class and the method createStmtDrawer inside this method you'll find a when statment. Add your Drawer as following private fun createStmtDrawer(type: String, list: Collection<String>, seed: Long, model: Model, minPropOcc: Int, maxPropertyLimit: Int ): StmtDrawer { //Add your new statement drawer type here to the `when` clause return when{ type.lowercase(Locale.getDefault()) == \"allowlist\" -> AllowListDrawer(list, seed, model, minPropOcc, maxPropertyLimit) type == \"MyDrawer2000\" -> MyDrawer2000(list, seed, model, minPropOcc, maxPropertyLimit) else -> BlockListDrawer(list, seed, model, minPropOcc, maxPropertyLimit) } } that is it. You can now use your Statement Drawer from the Configuration file.","title":"Add your Drawer to the Configuration"},{"location":"develop/guidelines/","text":"This sections explains some proposed guildelines for developing glisten Releases Glisten is setup to make it pretty easy to create a new release. Hence the main branch is restricted and can only be used using Pull Requests. Each Pull Request will trigger a new release. Thus the PR will check if everything is setup (e.g. new version no.) so we can smoothly release. The release will trigger that the mkdocs Documentation will also be build and copied to gh-pages . In which branch to work? The main developing branch is develop as main is declared as the release branch. However if multiple people working one one branch for different features might get annoying, we propose to use a new branch called feature/MYFEATURE and merge into develop using a PR when the code is tested and finished. Code of Conduct & Contribution Our Code of Conduct: https://github.com/dice-group/glisten/blob/main/CODE_OF_CONDUCT.md Our Contributiosn Readme: https://github.com/dice-group/glisten/blob/main/CONTRIBUTING.md","title":"Guidelines"},{"location":"develop/guidelines/#releases","text":"Glisten is setup to make it pretty easy to create a new release. Hence the main branch is restricted and can only be used using Pull Requests. Each Pull Request will trigger a new release. Thus the PR will check if everything is setup (e.g. new version no.) so we can smoothly release. The release will trigger that the mkdocs Documentation will also be build and copied to gh-pages .","title":"Releases"},{"location":"develop/guidelines/#in-which-branch-to-work","text":"The main developing branch is develop as main is declared as the release branch. However if multiple people working one one branch for different features might get annoying, we propose to use a new branch called feature/MYFEATURE and merge into develop using a PR when the code is tested and finished.","title":"In which branch to work?"},{"location":"develop/guidelines/#code-of-conduct-contribution","text":"Our Code of Conduct: https://github.com/dice-group/glisten/blob/main/CODE_OF_CONDUCT.md Our Contributiosn Readme: https://github.com/dice-group/glisten/blob/main/CONTRIBUTING.md","title":"Code of Conduct &amp; Contribution"},{"location":"develop/hobbit/","text":"In this section we explain how to add your benchmark to Hobbit. Be aware that you need a reachable server where you can store your target, sources and linked dataset. Add your Benchmark clone the glisten repo from https://github.com/dice-group/Glisten git clone https://github.com/dice-group/Glisten Create Configuration Now go into raki-hobbit/docker and edit the data_config.yml file and add your system as follows configurations: - name: \"MyBenchmark\" linksUrlZip: \"https://hobbitdata.informatik.uni-leipzig.de/glisten/bp_links.zip\" targetUrlZip: \"https://hobbitdata.informatik.uni-leipzig.de/glisten/bp_targets.zip\" sources: - \"https://hobbitdata.informatik.uni-leipzig.de/glisten/birthPlace.nt\" trueStmtDrawerOpt: stmtDrawerType: \"allowlist\" list: - \"http://dbpedia.org/ontology/birthPlace\" falseStmtDrawerOpt: stmtDrawerType: \"allowlist\" list: - \"http://dbpedia.org/ontology/birthPlace\" namespaces: - \"http://dbpedia.org/ontology/\" Set your benchmark name (e.g MyBenchmark) Set the target url zip file containing all target files. set the Linked datasets url zip file containing all linked datasets. These need to contain a file for each target named SOURCENAME_TARGETNAME.nt which contains the target dataset and the links between the specified source dataset and the target dataset. Add the links to your sources. And for the fact checking add the trueStmtDrawerOpt which desribes how to retrieve positive facts, as well as the falseStmtDrawerOpt . Set the stmtDrawerType to either allowlist or blocklist depending on how you want to generate the facts and add a list parameter which describes the allowlist (resp blocklist ) Finally add namespaces you want to use (mainly for performance usage) Update Docker container now you nede to update the docker container by simply executing ./build_docker.sh Change the Benchmark.ttl file Now go to https://git.project-hobbit.eu/glisten/benchmark/ and edit the benchmark.ttl file and add glisten:MyBenchmark a glisten:Datasets; rdfs:label \"MyBenchmark\"@en; rdfs:comment \"MyBenchmark Description\"@en . Be aware that the rdfs:label needs to be the same as the name in the configurations. Now you should be able to use your benchmark in Hobbit. Make sure that the corresponding links in the configuration are accessable.","title":"Hobbit"},{"location":"develop/hobbit/#add-your-benchmark","text":"clone the glisten repo from https://github.com/dice-group/Glisten git clone https://github.com/dice-group/Glisten","title":"Add your Benchmark"},{"location":"develop/hobbit/#create-configuration","text":"Now go into raki-hobbit/docker and edit the data_config.yml file and add your system as follows configurations: - name: \"MyBenchmark\" linksUrlZip: \"https://hobbitdata.informatik.uni-leipzig.de/glisten/bp_links.zip\" targetUrlZip: \"https://hobbitdata.informatik.uni-leipzig.de/glisten/bp_targets.zip\" sources: - \"https://hobbitdata.informatik.uni-leipzig.de/glisten/birthPlace.nt\" trueStmtDrawerOpt: stmtDrawerType: \"allowlist\" list: - \"http://dbpedia.org/ontology/birthPlace\" falseStmtDrawerOpt: stmtDrawerType: \"allowlist\" list: - \"http://dbpedia.org/ontology/birthPlace\" namespaces: - \"http://dbpedia.org/ontology/\" Set your benchmark name (e.g MyBenchmark) Set the target url zip file containing all target files. set the Linked datasets url zip file containing all linked datasets. These need to contain a file for each target named SOURCENAME_TARGETNAME.nt which contains the target dataset and the links between the specified source dataset and the target dataset. Add the links to your sources. And for the fact checking add the trueStmtDrawerOpt which desribes how to retrieve positive facts, as well as the falseStmtDrawerOpt . Set the stmtDrawerType to either allowlist or blocklist depending on how you want to generate the facts and add a list parameter which describes the allowlist (resp blocklist ) Finally add namespaces you want to use (mainly for performance usage)","title":"Create Configuration"},{"location":"develop/hobbit/#update-docker-container","text":"now you nede to update the docker container by simply executing ./build_docker.sh","title":"Update Docker container"},{"location":"develop/hobbit/#change-the-benchmarkttl-file","text":"Now go to https://git.project-hobbit.eu/glisten/benchmark/ and edit the benchmark.ttl file and add glisten:MyBenchmark a glisten:Datasets; rdfs:label \"MyBenchmark\"@en; rdfs:comment \"MyBenchmark Description\"@en . Be aware that the rdfs:label needs to be the same as the name in the configurations. Now you should be able to use your benchmark in Hobbit. Make sure that the corresponding links in the configuration are accessable.","title":"Change the Benchmark.ttl file"},{"location":"usage/configuration/","text":"There are two Configurations in Glisten. The Benchmark Configuration The Evaluation Parameters The Benchmark Configuration consists of all the information for one benchmark (datasets, namespaces, fact generation basics). The Evaluation Parameter define the specifics to use inside the benchmark (seed for random stuff, how many facts should be generated etc.) We will discuss both in the following and explain what you need to know. Prelimineries To understand the Configuration let's dive a little into what we need. As a base: We need one or more source datasets several target datasets (some better fitting to the source/s than others) for each target dataset, the links between the source and the target dataset. (For simplicity these linked target dataset contain the target as well and not just the links) For fact checking: A triplestore where we can load the datasets in how many true and false facts should be generated which seed to use (for reproducibility) Which facts we want to consider Some namespaces for performance reasons Which facts we want to consider Glisten currently implements two types on how to retrieve facts. Random Properties An Allow List A Block List The Allow list contains several predicates which are allowed, the block list contains several predicates which aren't allowed. The random properties will get random properties from the source model, where each property has to be in the namespace in the provided list. These list represent the base. Further on we can specify that a property needs to occur at least N times in the source dataset to be actually considered. True Facts will be then just drawn randomly based upon thes constrains. False Facts will be retrieved randomly, with the addition that the object of that Fact/Statement will be mutated to an object that exists in the source dataset with the property, but the Fact still remains false wrt to the source. Benchmark Configuration To now create the benchmark configuration we use the YAML format. The configurations file contains several configurations and the Idea is that, you can add your configuration for anyone to use, without specifing certain details which are then based in the evaluation parameters. So each configuration contains only the benchmark name The URL to a ZIP file containing all targets (To send to the recommendation system) The URL to a ZIP file containing all linked target datasets (For internal use) The URLs of each source to evaluate against The namespaces to use For the true fact generation: Which List type to use (randomProperties, allow list or block list) Which predicates to consider For the false fact generation: Which List type to use (randomProperties, allow list or block list) Which predicates to consider The configuration then looks like the following example: configurations: - name: \"test_benchmark\" linksUrlZip: \"file:///path/to/links.zip\" targetUrlZip: \"file:///path/to/targets.zip\" sources: - \"file:///path/to/source1.nt\" trueStmtDrawerOpt: stmtDrawerType: \"allowlist\" list: - \"http://dbpedia.org/ontology/mythology\" - \"http://dbpedia.org/ontology/creator\" falseStmtDrawerOpt: stmtDrawerType: \"blocklist\" list: - \"http://dbpedia.org/ontology/mythology\" - \"http://dbpedia.org/ontology/creator\" namespaces: - \"http://dbpedia.org/ontology/\" The benchmark name is test_benchmark and the URL links are both locally (you can use https:// as well and Glisten will download the zips). The source is listed using a local URL as well (again would work perfectly fine with an online one) Next the trueStmtDrawerOpt and falseStmtDrawerOpt represent the true and false fact generation (true/false statement fact drawer options) It defined the statement drawer type ( randomProperties , allowlist or blocklist ) and the list of predicates in this list. Further on it defines the namespaces to be used which in our case is only http://dbpedia.org/ontology/ . This will boost the performance immense on datasets like DBpedia Evaluation Parameters The evaluation parameters can be defined defined by running the benchmark itself (F.e. as CLI arguments) The parameters are listed in the following table Name Description Default seed The seed to use for any random acitivity Will be random, but logged out for reproduciblity minimum property occurrences The minimum amount a property has to occur in the dataset to be considered for fact generation 10 max property limit The max amount a property is retrieved for fact generation, if more facts are available will choose randomly the amount of facts. 30 max recommendations Only check against the Top N recommendations (For performance reasons) 10 Scorer Algorithm The Fact Checking algorithm. Currently implemented: [COPAAL, SampleCOPAAL, SampleCOPAAL_AvgScore, COPAAL_AvgScore, SampleCopaal_RootMeanSquare, Copaal_RootMeanSquare] COPAAL Scorer threshold The threshold to consider a true fact actually true. F.e. the scorer might state the true fact at 0.01, but that is not sufficient, we want at least a score of 0.2 to consider the fact true. This will put true facts behind false facts who are mostly at 0.0. This is more for debugging purposes . 0.0 number Of True Statements The no. of true facts to generate 5 number Of False Statements The no. of false facts to generate 5 benchmarkName The benchmark to use inside the configuration file test_benchmark config file The Benchmark Configuration File described above data_config.yml order file see below. random order rdf Endpoint The SPARQL endpoint to use to load the datasets into and query for fact checking - What is the order file The order file is an optional argument to use to specify the order your recommendation system would provide (If you're using the CLI executor). Each target dataset name should be listed inside this order file, seperated by new lines. The dataset on top is the best recommended dataset and the last dataset is the lowest recommended dataset. With this file you can execute the recommendation outside of Glisten and just check how well the produced recommendation ranking is. If you want to check your system directly you want to use our Hobbit Implementation instead.","title":"Configuration"},{"location":"usage/configuration/#prelimineries","text":"To understand the Configuration let's dive a little into what we need. As a base: We need one or more source datasets several target datasets (some better fitting to the source/s than others) for each target dataset, the links between the source and the target dataset. (For simplicity these linked target dataset contain the target as well and not just the links) For fact checking: A triplestore where we can load the datasets in how many true and false facts should be generated which seed to use (for reproducibility) Which facts we want to consider Some namespaces for performance reasons","title":"Prelimineries"},{"location":"usage/configuration/#which-facts-we-want-to-consider","text":"Glisten currently implements two types on how to retrieve facts. Random Properties An Allow List A Block List The Allow list contains several predicates which are allowed, the block list contains several predicates which aren't allowed. The random properties will get random properties from the source model, where each property has to be in the namespace in the provided list. These list represent the base. Further on we can specify that a property needs to occur at least N times in the source dataset to be actually considered. True Facts will be then just drawn randomly based upon thes constrains. False Facts will be retrieved randomly, with the addition that the object of that Fact/Statement will be mutated to an object that exists in the source dataset with the property, but the Fact still remains false wrt to the source.","title":"Which facts we want to consider"},{"location":"usage/configuration/#benchmark-configuration","text":"To now create the benchmark configuration we use the YAML format. The configurations file contains several configurations and the Idea is that, you can add your configuration for anyone to use, without specifing certain details which are then based in the evaluation parameters. So each configuration contains only the benchmark name The URL to a ZIP file containing all targets (To send to the recommendation system) The URL to a ZIP file containing all linked target datasets (For internal use) The URLs of each source to evaluate against The namespaces to use For the true fact generation: Which List type to use (randomProperties, allow list or block list) Which predicates to consider For the false fact generation: Which List type to use (randomProperties, allow list or block list) Which predicates to consider The configuration then looks like the following example: configurations: - name: \"test_benchmark\" linksUrlZip: \"file:///path/to/links.zip\" targetUrlZip: \"file:///path/to/targets.zip\" sources: - \"file:///path/to/source1.nt\" trueStmtDrawerOpt: stmtDrawerType: \"allowlist\" list: - \"http://dbpedia.org/ontology/mythology\" - \"http://dbpedia.org/ontology/creator\" falseStmtDrawerOpt: stmtDrawerType: \"blocklist\" list: - \"http://dbpedia.org/ontology/mythology\" - \"http://dbpedia.org/ontology/creator\" namespaces: - \"http://dbpedia.org/ontology/\" The benchmark name is test_benchmark and the URL links are both locally (you can use https:// as well and Glisten will download the zips). The source is listed using a local URL as well (again would work perfectly fine with an online one) Next the trueStmtDrawerOpt and falseStmtDrawerOpt represent the true and false fact generation (true/false statement fact drawer options) It defined the statement drawer type ( randomProperties , allowlist or blocklist ) and the list of predicates in this list. Further on it defines the namespaces to be used which in our case is only http://dbpedia.org/ontology/ . This will boost the performance immense on datasets like DBpedia","title":"Benchmark Configuration"},{"location":"usage/configuration/#evaluation-parameters","text":"The evaluation parameters can be defined defined by running the benchmark itself (F.e. as CLI arguments) The parameters are listed in the following table Name Description Default seed The seed to use for any random acitivity Will be random, but logged out for reproduciblity minimum property occurrences The minimum amount a property has to occur in the dataset to be considered for fact generation 10 max property limit The max amount a property is retrieved for fact generation, if more facts are available will choose randomly the amount of facts. 30 max recommendations Only check against the Top N recommendations (For performance reasons) 10 Scorer Algorithm The Fact Checking algorithm. Currently implemented: [COPAAL, SampleCOPAAL, SampleCOPAAL_AvgScore, COPAAL_AvgScore, SampleCopaal_RootMeanSquare, Copaal_RootMeanSquare] COPAAL Scorer threshold The threshold to consider a true fact actually true. F.e. the scorer might state the true fact at 0.01, but that is not sufficient, we want at least a score of 0.2 to consider the fact true. This will put true facts behind false facts who are mostly at 0.0. This is more for debugging purposes . 0.0 number Of True Statements The no. of true facts to generate 5 number Of False Statements The no. of false facts to generate 5 benchmarkName The benchmark to use inside the configuration file test_benchmark config file The Benchmark Configuration File described above data_config.yml order file see below. random order rdf Endpoint The SPARQL endpoint to use to load the datasets into and query for fact checking -","title":"Evaluation Parameters"},{"location":"usage/configuration/#what-is-the-order-file","text":"The order file is an optional argument to use to specify the order your recommendation system would provide (If you're using the CLI executor). Each target dataset name should be listed inside this order file, seperated by new lines. The dataset on top is the best recommended dataset and the last dataset is the lowest recommended dataset. With this file you can execute the recommendation outside of Glisten and just check how well the produced recommendation ranking is. If you want to check your system directly you want to use our Hobbit Implementation instead.","title":"What is the order file"},{"location":"usage/getting-started/","text":"We will explain how to get started with using Glisten in this section. There are two ways you can use Glisten as a benchmarking tool Using the local CLI interface Using Hobbit In this case we will show you how you can execute a benchmark using the CLI interface. If you want to use Hobbit, have a look at our [Hobbit Usage] documentation. Build the code First thing to do is to build the glisten code. Download git clone https://github.com/dice-group/Glisten cd Glisten Build Now we can build our code using maven mvn clean package If you encounter a problem telling you that Corraborative-0.2.0.jar cannot be retrieved, we supplied the jar file and the library with this code. Simply copy the file structure in ./lib/repository/ to ~/.m2/repository/ and maven should buid Glisten just fine. Execute Now that Glisten is build we can execute it. For clarification let's move the execution jar file to our current directory mv target/glisten-test-$RELEASE_VERSION ./ In the following steps the execution of the benchmark is declared. However you can use the help parameter to get some additional information on what you can change in your execution Usage: glisten-test [-hV] [--clean-up] [-c=<configFile>] [-F=<numberOfFalseStatements>] [-m=<maxRecommendations>] [--max-property-limit=<maxPropertyLimit>] [--min-prop-occ=<minPropOcc>] [-N=<benchmarkName>] [-o=<orderFile>] [-s=<scorerAlgorithm>] [-S=<seed>] [--sample-size=<sampleSize>] [-t=<threshold>] [-T=<numberOfTrueStatements>] <rdfEndpoint> Executes the glisten workflow without Hobbit and prints the ROC curve at the end. Mostly useful for debugging. Uses the load_triplestore.sh script file to upload datasets to the triplestore. <rdfEndpoint> the rdf endpoint to use -c, --config=<configFile> the config file for glisten. Default: data_config.yml --clean-up if set, will remove the testing directory which includes all downloaded and extracted datasets. -F, --no-of-false-stmts=<numberOfFalseStatements> the no. of false statements to generate. Default=5 -h, --help Show this help message and exit. -m, --max-recommendations=<maxRecommendations> the no. of max recommendations, 0 or lower means that all recommendations will be looked at. Default=10 --max-property-limit=<maxPropertyLimit> the maximum a property is allowed to be added for performance reasons. Default=30 --min-prop-occ=<minPropOcc> the minimum a property has to occur to be considered for the fact generation. Default=10 -N, --benchmark-name=<benchmarkName> The name of the benchmark to use. Name is specified inside the given configuration file. -o, --order-file=<orderFile> A file containing the order of the recommendations, if not set, will be random -s, --scorer=<scorerAlgorithm> The Scorer algorithm to use. Algorithms: [Copaal, SampleCopaal, Copaal_RootMeanSquare, SampleCopaal_RootMeanSquare, Copaal_AvgScore, SampleCopaal_AvgScore] -S, --seed=<seed> the seed to use for anything random we do. Default is random --sample-size=<sampleSize> the sample size to use if Scorer uses samples. Default=30 -t, --scorer-threshold=<threshold> the threshold to use inside the scorer. A true fact needs to be better than the threshold. Default=0.0 -T, --no-of-true-stmts=<numberOfTrueStatements> the no. of true statements to generate. Default=5 -V, --version Print version information and exit. For more information on each parameter have a look at our [Configuration] Execute a benchmark Now that you can execute glisten, we can execute a benchmark we want to use. The example.yml file which was delivered with the code contains some pre defined benchmarks we can use. (Be aware that some of these files might be huge and you need quite a bit of RAM for these to be executed.) If you want to build your own benchmark checkout: [How to create a Benchmark] let's execute a benchmark Choose a benchmark from the example.yml (we will use MySimpleBenchmark ) Setup your recommendation file Prepare a triplestore to use Execute the benchmark As stated we will use the MySimpleBenchmark benchmark. Setup the recommendation file If you don't want to use a recommendation system, and just checking out glisten, you can ignore this step, it will then use a random order. At this moment the CLI interface doesn't handle recommendations systems in itself (please refer to the Hobbit version for that), however you can execute the recommendations on your system yourself and provide the order (highest recommendation at top, lowest at the bottom) of the recommendations. In our case we have the following 4 target datasets, the recommendation system should rate wrt the source dataset change_this.nt . change_this1.nt change_this2.nt change_this3.nt change_this4.nt Either create a file with each line holding one dataset name. And the stated order will be used. Be aware that the dataset on the top should be the best recommendation. we will call this file the order file and store it under the name recommendations_order.txt . Prepare a triplestore to use You can choose any triplestore you want, the only important thing is, that it needs to be able to update the dataset on the fly with rather big files. (We will not use big files in our example benchmark, but you get the gist.) If you choose one and set it up as you want, start it. Now we only need to change the load_triplestore.sh script which is responsible for loading additional datasets into the triplestore. The script is setup for a Virtuoso instance with the isql port at 1111. To change the script it is important to note, that the script will get two arguments, the first describing the path to the current file, and the file name. E.g 1= /path/to/ and 2= fileName.nt If your choosen triplestore cannot update the database on the fly, you can use some tricks to do this, like stopping the server, updating the database and starting the server again. For no we will assume that the SPARQL endpoint is at http://localhost:8890/sparql Execute the benchmark Now that we have glisten build, a benchmark choosen, the triplestore set up and optionally an order file, we can execute the benchmark java -jar glisten-test-$RELEASE_VERSION.jar -o recommendations_order.txt -N MySimpleBenchmark http://localhost:8890/sparql or if you do not have an order file java -jar glisten-test-$RELEASE_VERSION.jar -N MySimpleBenchmark http://localhost:8890/sparql This execution will download and unzip all targets and links for the MySimpleBenchmark to the testing folder and executes the benchmark. There should be some log output and at the end you'll get the ROC Curve and the Area under the Curve for the given order (or a random one, if you used no order file).","title":"Getting started"},{"location":"usage/getting-started/#build-the-code","text":"First thing to do is to build the glisten code.","title":"Build the code"},{"location":"usage/getting-started/#download","text":"git clone https://github.com/dice-group/Glisten cd Glisten","title":"Download"},{"location":"usage/getting-started/#build","text":"Now we can build our code using maven mvn clean package If you encounter a problem telling you that Corraborative-0.2.0.jar cannot be retrieved, we supplied the jar file and the library with this code. Simply copy the file structure in ./lib/repository/ to ~/.m2/repository/ and maven should buid Glisten just fine.","title":"Build"},{"location":"usage/getting-started/#execute","text":"Now that Glisten is build we can execute it. For clarification let's move the execution jar file to our current directory mv target/glisten-test-$RELEASE_VERSION ./ In the following steps the execution of the benchmark is declared. However you can use the help parameter to get some additional information on what you can change in your execution Usage: glisten-test [-hV] [--clean-up] [-c=<configFile>] [-F=<numberOfFalseStatements>] [-m=<maxRecommendations>] [--max-property-limit=<maxPropertyLimit>] [--min-prop-occ=<minPropOcc>] [-N=<benchmarkName>] [-o=<orderFile>] [-s=<scorerAlgorithm>] [-S=<seed>] [--sample-size=<sampleSize>] [-t=<threshold>] [-T=<numberOfTrueStatements>] <rdfEndpoint> Executes the glisten workflow without Hobbit and prints the ROC curve at the end. Mostly useful for debugging. Uses the load_triplestore.sh script file to upload datasets to the triplestore. <rdfEndpoint> the rdf endpoint to use -c, --config=<configFile> the config file for glisten. Default: data_config.yml --clean-up if set, will remove the testing directory which includes all downloaded and extracted datasets. -F, --no-of-false-stmts=<numberOfFalseStatements> the no. of false statements to generate. Default=5 -h, --help Show this help message and exit. -m, --max-recommendations=<maxRecommendations> the no. of max recommendations, 0 or lower means that all recommendations will be looked at. Default=10 --max-property-limit=<maxPropertyLimit> the maximum a property is allowed to be added for performance reasons. Default=30 --min-prop-occ=<minPropOcc> the minimum a property has to occur to be considered for the fact generation. Default=10 -N, --benchmark-name=<benchmarkName> The name of the benchmark to use. Name is specified inside the given configuration file. -o, --order-file=<orderFile> A file containing the order of the recommendations, if not set, will be random -s, --scorer=<scorerAlgorithm> The Scorer algorithm to use. Algorithms: [Copaal, SampleCopaal, Copaal_RootMeanSquare, SampleCopaal_RootMeanSquare, Copaal_AvgScore, SampleCopaal_AvgScore] -S, --seed=<seed> the seed to use for anything random we do. Default is random --sample-size=<sampleSize> the sample size to use if Scorer uses samples. Default=30 -t, --scorer-threshold=<threshold> the threshold to use inside the scorer. A true fact needs to be better than the threshold. Default=0.0 -T, --no-of-true-stmts=<numberOfTrueStatements> the no. of true statements to generate. Default=5 -V, --version Print version information and exit. For more information on each parameter have a look at our [Configuration]","title":"Execute"},{"location":"usage/getting-started/#execute-a-benchmark","text":"Now that you can execute glisten, we can execute a benchmark we want to use. The example.yml file which was delivered with the code contains some pre defined benchmarks we can use. (Be aware that some of these files might be huge and you need quite a bit of RAM for these to be executed.) If you want to build your own benchmark checkout: [How to create a Benchmark] let's execute a benchmark Choose a benchmark from the example.yml (we will use MySimpleBenchmark ) Setup your recommendation file Prepare a triplestore to use Execute the benchmark As stated we will use the MySimpleBenchmark benchmark.","title":"Execute a benchmark"},{"location":"usage/getting-started/#setup-the-recommendation-file","text":"If you don't want to use a recommendation system, and just checking out glisten, you can ignore this step, it will then use a random order. At this moment the CLI interface doesn't handle recommendations systems in itself (please refer to the Hobbit version for that), however you can execute the recommendations on your system yourself and provide the order (highest recommendation at top, lowest at the bottom) of the recommendations. In our case we have the following 4 target datasets, the recommendation system should rate wrt the source dataset change_this.nt . change_this1.nt change_this2.nt change_this3.nt change_this4.nt Either create a file with each line holding one dataset name. And the stated order will be used. Be aware that the dataset on the top should be the best recommendation. we will call this file the order file and store it under the name recommendations_order.txt .","title":"Setup the recommendation file"},{"location":"usage/getting-started/#prepare-a-triplestore-to-use","text":"You can choose any triplestore you want, the only important thing is, that it needs to be able to update the dataset on the fly with rather big files. (We will not use big files in our example benchmark, but you get the gist.) If you choose one and set it up as you want, start it. Now we only need to change the load_triplestore.sh script which is responsible for loading additional datasets into the triplestore. The script is setup for a Virtuoso instance with the isql port at 1111. To change the script it is important to note, that the script will get two arguments, the first describing the path to the current file, and the file name. E.g 1= /path/to/ and 2= fileName.nt If your choosen triplestore cannot update the database on the fly, you can use some tricks to do this, like stopping the server, updating the database and starting the server again. For no we will assume that the SPARQL endpoint is at http://localhost:8890/sparql","title":"Prepare a triplestore to use"},{"location":"usage/getting-started/#execute-the-benchmark","text":"Now that we have glisten build, a benchmark choosen, the triplestore set up and optionally an order file, we can execute the benchmark java -jar glisten-test-$RELEASE_VERSION.jar -o recommendations_order.txt -N MySimpleBenchmark http://localhost:8890/sparql or if you do not have an order file java -jar glisten-test-$RELEASE_VERSION.jar -N MySimpleBenchmark http://localhost:8890/sparql This execution will download and unzip all targets and links for the MySimpleBenchmark to the testing folder and executes the benchmark. There should be some log output and at the end you'll get the ROC Curve and the Area under the Curve for the given order (or a random one, if you used no order file).","title":"Execute the benchmark"},{"location":"usage/hobbit/","text":"In this section we explain how to execute a benchmark in Hobbit, what parameters you can use and how to add your own recommendation system to be tested in Hobbit. How to execute a Benchmark Go to https://main.project-hobbit.eu and login. Got to Benchmarks and choose Glisten Benchmark . Now choose your System to test, the benchmark you want to execute and all the parameters yo use. Parameters Name Description benchmarkName The benchmark to use inside the configuration file seed The seed to use for any random acitivity minimum property occurrences The minimum amount a property has to occur in the dataset to be considered for fact generation max property limit The max amount a property is retrieved for fact generation, if more facts are available will choose randomly the amount of facts. max recommendations Only check against the Top N recommendations (For performance reasons) Scorer Algorithm The Fact Checking algorithm. Currently implemented: [COPAAL, SampleCOPAAL] number Of True Statements The no. of true facts to generate number Of False Statements The no. of false facts to generate How to add my own System To add your own recommendation system to Hobbits Glisten Benchmark you need to create a glisten system wrapper Create a repository add https://git.project-hobbit.eu/ Add you system docker container to this repository create a glisten system wrapper For this section we assume you simply use the Glisen repo at https://github.com/dice-group/Glisten and adding your system to the raki-system-adapter module. Create a Class (we call this MySystem in the org.dice_group.glisten.hobbit.systems package for now and implement it using Kotlin) package org.dice_group.glisten.hobbit.systems import org.dice_group.glisten.hobbit.systems.AbstractGlistenHobbitSystem import java.io.File /** * Simple test system which just randomly sets scores from 0.0 to 1.0 */ class MySystem : AbstractGlistenHobbitSystem() { override fun init(){ super.init() //You can init your system here } /** * Given a source file and a list of target files, the system shall create a score * for each target file. * * The score represents how fitting the target is for the source file (aka, is it a good recommendation) * * It shall return the mapping File->Score * * @param source The source to generate the recommendation scores with * @param targets The targets to generate the recommendations scores for * @return The mapping from target file to recommendation score */ override fun generateRecommendationScores(source: File, targets: ArrayList<File>): List<Pair<File, Double>> { val ret = mutableListOf<Pair<File, Double>>() targets.forEach{ target -> //TODO create your score for this target here val score = 0.0 ret.add(Pair(target, score)) } return ret } } Thats it, implement the TODOs and your system is ready to go. Create a repository add https://git.project-hobbit.eu/ Go to https://git.project-hobbit.eu/ and create a new project. Add a file called system.ttl containing the following @prefix rdfs: <http://www.w3.org/2000/01/rdf-schema#> . @prefix hobbit: <http://w3id.org/hobbit/vocab#> . @prefix glisten: <http://w3id.org/glisten/hobbit/vocab#> . raki:MySystem a hobbit:SystemInstance; rdfs:label \"MySystem\"@en; rdfs:comment \"Description of your system.\"@en; hobbit:imageName \"git.project-hobbit.eu:4567/YOUR_USERNAME/YOUR_PROJECT_NAME\"; hobbit:implementsAPI glisten:Glisten-API . Add you system docker container to this repository Now we need to create a Dockercontainer for the system and push that systme to the hobbit registry. Create a Dockerfile in your project FROM java ADD target/glisten-test-1.0.0-SNAPSHOT.jar /glisten/systems.jar WORKDIR /glisten CMD java -cp systems.jar org.hobbit.core.run.ComponentStarter org.dice_group.glisten.hobbit.systems.MySystemKt now build the project mvn clean package -P shaded build and push the container to the hobbit registry. docker login git.project-hobbit.eu:4567 docker build -t git.project-hobbit.eu:4567/YOUR_USERNAME/YOUR_PROJECT_NAME . docker push git.project-hobbit.eu:4567/YOUR_USERNAME/YOUR_PROJECT_NAME Now you should be able to access your system on Hobbit.","title":"Hobbit"},{"location":"usage/hobbit/#how-to-execute-a-benchmark","text":"Go to https://main.project-hobbit.eu and login. Got to Benchmarks and choose Glisten Benchmark . Now choose your System to test, the benchmark you want to execute and all the parameters yo use.","title":"How to execute a Benchmark"},{"location":"usage/hobbit/#parameters","text":"Name Description benchmarkName The benchmark to use inside the configuration file seed The seed to use for any random acitivity minimum property occurrences The minimum amount a property has to occur in the dataset to be considered for fact generation max property limit The max amount a property is retrieved for fact generation, if more facts are available will choose randomly the amount of facts. max recommendations Only check against the Top N recommendations (For performance reasons) Scorer Algorithm The Fact Checking algorithm. Currently implemented: [COPAAL, SampleCOPAAL] number Of True Statements The no. of true facts to generate number Of False Statements The no. of false facts to generate","title":"Parameters"},{"location":"usage/hobbit/#how-to-add-my-own-system","text":"To add your own recommendation system to Hobbits Glisten Benchmark you need to create a glisten system wrapper Create a repository add https://git.project-hobbit.eu/ Add you system docker container to this repository","title":"How to add my own System"},{"location":"usage/hobbit/#create-a-glisten-system-wrapper","text":"For this section we assume you simply use the Glisen repo at https://github.com/dice-group/Glisten and adding your system to the raki-system-adapter module. Create a Class (we call this MySystem in the org.dice_group.glisten.hobbit.systems package for now and implement it using Kotlin) package org.dice_group.glisten.hobbit.systems import org.dice_group.glisten.hobbit.systems.AbstractGlistenHobbitSystem import java.io.File /** * Simple test system which just randomly sets scores from 0.0 to 1.0 */ class MySystem : AbstractGlistenHobbitSystem() { override fun init(){ super.init() //You can init your system here } /** * Given a source file and a list of target files, the system shall create a score * for each target file. * * The score represents how fitting the target is for the source file (aka, is it a good recommendation) * * It shall return the mapping File->Score * * @param source The source to generate the recommendation scores with * @param targets The targets to generate the recommendations scores for * @return The mapping from target file to recommendation score */ override fun generateRecommendationScores(source: File, targets: ArrayList<File>): List<Pair<File, Double>> { val ret = mutableListOf<Pair<File, Double>>() targets.forEach{ target -> //TODO create your score for this target here val score = 0.0 ret.add(Pair(target, score)) } return ret } } Thats it, implement the TODOs and your system is ready to go.","title":"create a glisten system wrapper"},{"location":"usage/hobbit/#create-a-repository-add-httpsgitproject-hobbiteu","text":"Go to https://git.project-hobbit.eu/ and create a new project. Add a file called system.ttl containing the following @prefix rdfs: <http://www.w3.org/2000/01/rdf-schema#> . @prefix hobbit: <http://w3id.org/hobbit/vocab#> . @prefix glisten: <http://w3id.org/glisten/hobbit/vocab#> . raki:MySystem a hobbit:SystemInstance; rdfs:label \"MySystem\"@en; rdfs:comment \"Description of your system.\"@en; hobbit:imageName \"git.project-hobbit.eu:4567/YOUR_USERNAME/YOUR_PROJECT_NAME\"; hobbit:implementsAPI glisten:Glisten-API .","title":"Create a repository add https://git.project-hobbit.eu/"},{"location":"usage/hobbit/#add-you-system-docker-container-to-this-repository","text":"Now we need to create a Dockercontainer for the system and push that systme to the hobbit registry. Create a Dockerfile in your project FROM java ADD target/glisten-test-1.0.0-SNAPSHOT.jar /glisten/systems.jar WORKDIR /glisten CMD java -cp systems.jar org.hobbit.core.run.ComponentStarter org.dice_group.glisten.hobbit.systems.MySystemKt now build the project mvn clean package -P shaded build and push the container to the hobbit registry. docker login git.project-hobbit.eu:4567 docker build -t git.project-hobbit.eu:4567/YOUR_USERNAME/YOUR_PROJECT_NAME . docker push git.project-hobbit.eu:4567/YOUR_USERNAME/YOUR_PROJECT_NAME Now you should be able to access your system on Hobbit.","title":"Add you system docker container to this repository"},{"location":"usage/programmatically/","text":"In this section we go through Glisten as a library. We will discuss examples on how to use the Glisten core insider your application. If you want to exend glisten hava a look at our develop section. Glisten was programmed in Kotlin and is thus accessible in Java as well. It is advised to use Kotlin however. Add Glisten as a Maven dependency Add the github repository to your pom.xml <repository> <id>glisten-github</id> <name>Glisten Dice Group repository</name> <url>https://maven.pkg.github.com/dice-group/Glisten</url> </repository> Now add the dependency <dependency> <groupId>org.dice_group</groupId> <artifactId>Glisten</artifactId> <version>$RELEASE_VERSION</version> </dependency> Execute A benchmark To execute a benchmark programmatically you need to use the CoreEvaluator create a Configuration (we will load one from file and uses the configuration with the name testBenchmark) val conf = ConfigurationFactory.findCorrectConfiguration(\"/path/to/config.yaml\", \"testBenchmark\") create a Scorer Algorithm we want to use. (we will use Copaal for this one) val scorer : Scorer = Copaal(conf.namespaces) lets assume our triplestore is started at http://localhost:9999/sparql val rdfEndpoint = \"http://localhost:9999/sparql\" Be aware to load the datasets into the triplestore the script load_triplestore.sh is used with the arguments /path/to/ and file.nt change that script to so that it loads the given file into your triplestore. An example is provided for Virtuoso let's create the CoreEvaluator val evaluator = CoreEvaluator(conf, rdfEndpoint, scorer) init and download the files into the /tmp folder, we can choose a different one. evaluator.init(\"/tmp/\") now we need a source file and some recommendations for the source file we will use the first one inside our configuration val source = conf.sources[0] for our recommendations, we will just use a mock here, here is the part where you can set your recommendation system important is only that you create a list containing all recommendations which are listed in the zip file stated in the configuration under conf.targetUrlZip val recommendations = listOf(Pair(\"file:///path/to/target1\", 0.1), Pair(\"file:///path/to/target2\", 0.05)) Finally let's start the benchmark val auc = evaluator.getAUC(source, recommendations) println(\"AUC score is $auc\") Create some Facts If you want to create some facts, you'll need some Statement Drawers . These will retrieve some facts/statements based upon some constraints (e.g. An Allow list or a block list) Let's assume we want to use an Allow List for both val minPropertyOccurrences = 10 val maxPropertyLimit = 30 val seed = 123L //Create your Model here and load it properyl val model = ModelFactory.createDefaultModel() // Let's allow both the properties name and author val allowed = listOf(\"http://example.com/property/name\", \"http://example.com/property/author\") val trueDrawer = AllowListDrawer(allowed, seed, model, minPropertyOccurrences, maxPropertyLimit) val falseDrawer = AllowListDrawer(allowed, seed, model, minPropertyOccurrences, maxPropertyLimit) val facts = FactGenerator.createFacts(seed, 20, 10, trueDrawer, falseDrawer) That's it now you have list of Pairs containg facts and if the fact is true (1.0) or false (-1.0) Get some Scores If you just want some scores using the fact checker you can use the Scorer Algorithm directly. First we need some facts to score against and a SPARQL endpoint to use. For simplictly we assume that you created two Apache Jena RDF Statements. The first one is the trueStatement and the second one is the falseStatement . It doesn't matter too much, you just need a list of pairs containg statments and if they are true or false. val endpoint = \"http://localhost:9999/sparql\" //CHANGE THIS //Create some Pairs of Jena Statements and if they are true or false, we assume that you have a true and false statement created beforehand val facts = listOf(Pair(trueStatement, 1.0), Pair(falseStatement, -1.0)) With that we can now create our Scorer and execute the algorithm. We will use Copaal as the Scorer Algorithm. // Create our scorer val scorer : Scorer = ScorerFactory.createScorerOrDefault(\"Copaal\",conf.namespaces) //create our score val score : Double = scorer.getScore(endpoint, facts) Javadoc/KDoc You can find the KDoc/Dokka at here","title":"Programmatically"},{"location":"usage/programmatically/#add-glisten-as-a-maven-dependency","text":"Add the github repository to your pom.xml <repository> <id>glisten-github</id> <name>Glisten Dice Group repository</name> <url>https://maven.pkg.github.com/dice-group/Glisten</url> </repository> Now add the dependency <dependency> <groupId>org.dice_group</groupId> <artifactId>Glisten</artifactId> <version>$RELEASE_VERSION</version> </dependency>","title":"Add Glisten as a Maven dependency"},{"location":"usage/programmatically/#execute-a-benchmark","text":"To execute a benchmark programmatically you need to use the CoreEvaluator create a Configuration (we will load one from file and uses the configuration with the name testBenchmark) val conf = ConfigurationFactory.findCorrectConfiguration(\"/path/to/config.yaml\", \"testBenchmark\") create a Scorer Algorithm we want to use. (we will use Copaal for this one) val scorer : Scorer = Copaal(conf.namespaces) lets assume our triplestore is started at http://localhost:9999/sparql val rdfEndpoint = \"http://localhost:9999/sparql\" Be aware to load the datasets into the triplestore the script load_triplestore.sh is used with the arguments /path/to/ and file.nt change that script to so that it loads the given file into your triplestore. An example is provided for Virtuoso let's create the CoreEvaluator val evaluator = CoreEvaluator(conf, rdfEndpoint, scorer) init and download the files into the /tmp folder, we can choose a different one. evaluator.init(\"/tmp/\") now we need a source file and some recommendations for the source file we will use the first one inside our configuration val source = conf.sources[0] for our recommendations, we will just use a mock here, here is the part where you can set your recommendation system important is only that you create a list containing all recommendations which are listed in the zip file stated in the configuration under conf.targetUrlZip val recommendations = listOf(Pair(\"file:///path/to/target1\", 0.1), Pair(\"file:///path/to/target2\", 0.05)) Finally let's start the benchmark val auc = evaluator.getAUC(source, recommendations) println(\"AUC score is $auc\")","title":"Execute A benchmark"},{"location":"usage/programmatically/#create-some-facts","text":"If you want to create some facts, you'll need some Statement Drawers . These will retrieve some facts/statements based upon some constraints (e.g. An Allow list or a block list) Let's assume we want to use an Allow List for both val minPropertyOccurrences = 10 val maxPropertyLimit = 30 val seed = 123L //Create your Model here and load it properyl val model = ModelFactory.createDefaultModel() // Let's allow both the properties name and author val allowed = listOf(\"http://example.com/property/name\", \"http://example.com/property/author\") val trueDrawer = AllowListDrawer(allowed, seed, model, minPropertyOccurrences, maxPropertyLimit) val falseDrawer = AllowListDrawer(allowed, seed, model, minPropertyOccurrences, maxPropertyLimit) val facts = FactGenerator.createFacts(seed, 20, 10, trueDrawer, falseDrawer) That's it now you have list of Pairs containg facts and if the fact is true (1.0) or false (-1.0)","title":"Create some Facts"},{"location":"usage/programmatically/#get-some-scores","text":"If you just want some scores using the fact checker you can use the Scorer Algorithm directly. First we need some facts to score against and a SPARQL endpoint to use. For simplictly we assume that you created two Apache Jena RDF Statements. The first one is the trueStatement and the second one is the falseStatement . It doesn't matter too much, you just need a list of pairs containg statments and if they are true or false. val endpoint = \"http://localhost:9999/sparql\" //CHANGE THIS //Create some Pairs of Jena Statements and if they are true or false, we assume that you have a true and false statement created beforehand val facts = listOf(Pair(trueStatement, 1.0), Pair(falseStatement, -1.0)) With that we can now create our Scorer and execute the algorithm. We will use Copaal as the Scorer Algorithm. // Create our scorer val scorer : Scorer = ScorerFactory.createScorerOrDefault(\"Copaal\",conf.namespaces) //create our score val score : Double = scorer.getScore(endpoint, facts)","title":"Get some Scores"},{"location":"usage/programmatically/#javadockdoc","text":"You can find the KDoc/Dokka at here","title":"Javadoc/KDoc"}]}